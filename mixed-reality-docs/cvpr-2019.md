---
title: Visione artificiale applicazioni per il laboratorio di auricolari con realtà mista a CVPR 2019
description: Panoramica e pianificazione del workshop sulle applicazioni Visione artificiale per gli auricolari a realtà mista, da consegnare alla conferenza CVPR il 2019 giugno.
author: fbogo
ms.author: febogo
ms.date: 1/9/2019
ms.topic: article
keywords: evento, modalità di ricerca, cvpr, visione artificiale, ricerca, HoloLens
ms.openlocfilehash: 89d79bcef77043564e51faada940d2c71a6005e4
ms.sourcegitcommit: 2f600e5ad00cd447b180b0f89192b4b9d86bbc7e
ms.translationtype: MT
ms.contentlocale: it-IT
ms.lasthandoff: 06/15/2019
ms.locfileid: "67148704"
---
# <a name="computer-vision-applications-for-mixed-reality-headsets"></a><span data-ttu-id="99f3f-104">Visione artificiale applicazioni per cuffie con realtà mista</span><span class="sxs-lookup"><span data-stu-id="99f3f-104">Computer Vision Applications for Mixed Reality Headsets</span></span>

<span data-ttu-id="99f3f-105">Organizzata insieme a [CVPR 2019](http://cvpr2019.thecvf.com/)</span><span class="sxs-lookup"><span data-stu-id="99f3f-105">Organized in conjunction with [CVPR 2019](http://cvpr2019.thecvf.com/)</span></span>

<span data-ttu-id="99f3f-106">Long Beach (CA)</span><span class="sxs-lookup"><span data-stu-id="99f3f-106">Long Beach (CA)</span></span>

<span data-ttu-id="99f3f-107">17 giugno 2019 (pomeriggio)-Hyatt Regency F</span><span class="sxs-lookup"><span data-stu-id="99f3f-107">June 17, 2019 (Afternoon) - Hyatt Regency F</span></span>


## <a name="organizers"></a><span data-ttu-id="99f3f-108">Librerie</span><span class="sxs-lookup"><span data-stu-id="99f3f-108">Organizers</span></span>
* <span data-ttu-id="99f3f-109">Marc Pollefeys</span><span class="sxs-lookup"><span data-stu-id="99f3f-109">Marc Pollefeys</span></span>
* <span data-ttu-id="99f3f-110">Federica Bogo</span><span class="sxs-lookup"><span data-stu-id="99f3f-110">Federica Bogo</span></span>
* <span data-ttu-id="99f3f-111">Di Johannes Schönberger</span><span class="sxs-lookup"><span data-stu-id="99f3f-111">Johannes Schönberger</span></span>
* <span data-ttu-id="99f3f-112">Osman Ulusoy</span><span class="sxs-lookup"><span data-stu-id="99f3f-112">Osman Ulusoy</span></span>

## <a name="overview"></a><span data-ttu-id="99f3f-113">Panoramica</span><span class="sxs-lookup"><span data-stu-id="99f3f-113">Overview</span></span>

![Immagine del teaser](images/cvpr2019_teaser2.jpg)

<span data-ttu-id="99f3f-115">Gli auricolari per la realtà mista, ad esempio Microsoft HoloLens, diventano piattaforme potenti per sviluppare applicazioni di visione artificiale.</span><span class="sxs-lookup"><span data-stu-id="99f3f-115">Mixed reality headsets such as the Microsoft HoloLens are becoming powerful platforms to develop computer vision applications.</span></span> <span data-ttu-id="99f3f-116">La modalità di ricerca di HoloLens consente la ricerca di visione artificiale sul dispositivo fornendo l'accesso a tutti i flussi dei sensori di immagini RAW, inclusi Depth e IR.</span><span class="sxs-lookup"><span data-stu-id="99f3f-116">HoloLens Research Mode enables computer vision research on device by providing access to all raw image sensor streams -- including depth and IR.</span></span> <span data-ttu-id="99f3f-117">Poiché la modalità di ricerca è ora disponibile a partire dal 2018 maggio, verranno visualizzate diverse demo e applicazioni interessanti sviluppate per HoloLens.</span><span class="sxs-lookup"><span data-stu-id="99f3f-117">As Research Mode is now available since May 2018, we are starting to see several interesting demos and applications being developed for HoloLens.</span></span> 

<span data-ttu-id="99f3f-118">L'obiettivo di questo workshop è riunire gli studenti e i ricercatori interessati alla visione artificiale per le applicazioni di realtà mista.</span><span class="sxs-lookup"><span data-stu-id="99f3f-118">The goal of this workshop is to bring together students and researchers interested in computer vision for mixed reality applications.</span></span> <span data-ttu-id="99f3f-119">Il workshop fornirà una sede per condividere demo e applicazioni e imparare tra loro per creare o trasferire applicazioni in realtà mista.</span><span class="sxs-lookup"><span data-stu-id="99f3f-119">The workshop will provide a venue to share demos and applications, and learn from each other to build or port applications to mixed reality.</span></span> 

<span data-ttu-id="99f3f-120">Invitiamo gli invii per gli argomenti relativi al riconoscimento degli oggetti (incentrato sugli ego), al rilevamento di utenti e utenti, al riconoscimento delle attività, allo SLAM, alla ricostruzione 3D, alla comprensione della scena, alla localizzazione basata su sensori, alla navigazione e molto altro.</span><span class="sxs-lookup"><span data-stu-id="99f3f-120">We encourage submissions on the topics of (ego-centric) object recognition, hand and user tracking, activity recognition, SLAM, 3D reconstruction, scene understanding, sensor-based localization, navigation and more.</span></span>

## <a name="paper-submission"></a><span data-ttu-id="99f3f-121">Invio di carta</span><span class="sxs-lookup"><span data-stu-id="99f3f-121">Paper Submission</span></span>
* <span data-ttu-id="99f3f-122">Scadenza invio carta: 17 maggio</span><span class="sxs-lookup"><span data-stu-id="99f3f-122">Paper submission deadline: May 17</span></span>
* <span data-ttu-id="99f3f-123">Notifica agli autori: 24 maggio</span><span class="sxs-lookup"><span data-stu-id="99f3f-123">Notification to authors: May 24</span></span>

<span data-ttu-id="99f3f-124">Gli invii cartacei devono usare il modello CVPR e sono limitati a 4 pagine oltre ai riferimenti.</span><span class="sxs-lookup"><span data-stu-id="99f3f-124">Paper submissions should use the CVPR template and are limited to 4 pages plus references.</span></span> <span data-ttu-id="99f3f-125">Inoltre, invitiamo gli autori a inviare un video che mostra la propria applicazione.</span><span class="sxs-lookup"><span data-stu-id="99f3f-125">In addition, we encourage the authors to submit a video showcasing their application.</span></span>
<span data-ttu-id="99f3f-126">Si noti che sono consentiti invii di lavoro pubblicati in precedenza (incluso il lavoro accettato alla conferenza principale CVPR 2019).</span><span class="sxs-lookup"><span data-stu-id="99f3f-126">Note that submissions of previously published work are allowed (including work accepted to the main CVPR 2019 conference).</span></span> 

<span data-ttu-id="99f3f-127">Gli invii possono essere caricati in CMT: https://cmt3.research.microsoft.com/CVFORMR2019</span><span class="sxs-lookup"><span data-stu-id="99f3f-127">Submissions can be uploaded to the CMT: https://cmt3.research.microsoft.com/CVFORMR2019</span></span>

<span data-ttu-id="99f3f-128">Un subset di documenti verrà selezionato per la presentazione orale nel workshop.</span><span class="sxs-lookup"><span data-stu-id="99f3f-128">A subset of papers will be selected for oral presentation at the workshop.</span></span> <span data-ttu-id="99f3f-129">Tuttavia, si consiglia vivamente a tutti gli autori di presentare il proprio lavoro durante la sessione dimostrativa.</span><span class="sxs-lookup"><span data-stu-id="99f3f-129">However, we strongly encourage all the authors to present their work during the demo session.</span></span>


## <a name="schedule"></a><span data-ttu-id="99f3f-130">Pianificazione</span><span class="sxs-lookup"><span data-stu-id="99f3f-130">Schedule</span></span>
* <span data-ttu-id="99f3f-131">13:30-13:45: Benvenuti e apertura di commenti.</span><span class="sxs-lookup"><span data-stu-id="99f3f-131">13:30-13:45: Welcome and Opening Remarks.</span></span>
* <span data-ttu-id="99f3f-132">13:45-14:15: **Discussione introduttiva**: Prof. Marc Pollefeys, ETH Zurigo/Microsoft.</span><span class="sxs-lookup"><span data-stu-id="99f3f-132">13:45-14:15: **Keynote talk**: Prof. Marc Pollefeys, ETH Zurich/Microsoft.</span></span> <span data-ttu-id="99f3f-133">titolo Egocentrico Visione artificiale in HoloLens.</span><span class="sxs-lookup"><span data-stu-id="99f3f-133">Title: Egocentric Computer Vision on HoloLens.</span></span>
* <span data-ttu-id="99f3f-134">14:15-14:45: **Discussione introduttiva**: Prof. Kris Kitani, Carnegie Mellon University.</span><span class="sxs-lookup"><span data-stu-id="99f3f-134">14:15-14:45: **Keynote talk**: Prof. Kris Kitani, Carnegie Mellon University.</span></span> <span data-ttu-id="99f3f-135">titolo Attività egocentrico e previsione delle pose.</span><span class="sxs-lookup"><span data-stu-id="99f3f-135">Title: Egocentric Activity and Pose Forecasting.</span></span>
* <span data-ttu-id="99f3f-136">14:45-15:15: **Discussione introduttiva**: Dr. Yang Liu, California Institute of Technology.</span><span class="sxs-lookup"><span data-stu-id="99f3f-136">14:45-15:15: **Keynote talk**: Dr. Yang Liu, California Institute of Technology.</span></span> <span data-ttu-id="99f3f-137">titolo Potenziamento di un assistente cognitivo per la cecità con realtà aumentata.</span><span class="sxs-lookup"><span data-stu-id="99f3f-137">Title: Powering a Cognitive Assistant for the Blind with Augmented Reality.</span></span>
* <span data-ttu-id="99f3f-138">15:15-16:15: Coffee break e demo.</span><span class="sxs-lookup"><span data-stu-id="99f3f-138">15:15-16:15: Coffee break and demos.</span></span>
* <span data-ttu-id="99f3f-139">16:15-16:45: **Discussione introduttiva**: Prof. Kristen Graun, University of Texas presso Austin/Facebook AI Research.</span><span class="sxs-lookup"><span data-stu-id="99f3f-139">16:15-16:45: **Keynote talk**: Prof. Kristen Grauman, University of Texas at Austin/Facebook AI Research.</span></span> <span data-ttu-id="99f3f-140">titolo Interazione tra oggetti umani nel video di prima persona.</span><span class="sxs-lookup"><span data-stu-id="99f3f-140">Title: Human-object interaction in first-person video.</span></span>
* <span data-ttu-id="99f3f-141">16:45-17:15: Presentazioni orali:</span><span class="sxs-lookup"><span data-stu-id="99f3f-141">16:45-17:15: Oral presentations:</span></span>
    * <span data-ttu-id="99f3f-142">La registrazione è stata semplificata per la navigazione ortopedica autonoma con HoloLens.</span><span class="sxs-lookup"><span data-stu-id="99f3f-142">Registration made easy - standalone orthopedic navigation with HoloLens.</span></span> <span data-ttu-id="99f3f-143">F.</span><span class="sxs-lookup"><span data-stu-id="99f3f-143">F.</span></span> <span data-ttu-id="99f3f-144">Liebmann, S. Roner, M. von Atzigen, F. Wanivenhaus, C. Neuhaus, J. Spirig, D. Scaramuzza, R. Sutter, J. Snedeker, M. figo, P. Furnstahl.</span><span class="sxs-lookup"><span data-stu-id="99f3f-144">Liebmann, S. Roner, M. von Atzigen, F. Wanivenhaus, C. Neuhaus, J. Spirig, D. Scaramuzza, R. Sutter, J. Snedeker, M. Farshad, P. Furnstahl.</span></span>
    * <span data-ttu-id="99f3f-145">Imparare a usare lo stereo con una HoloLens.</span><span class="sxs-lookup"><span data-stu-id="99f3f-145">Learning stereo by walking around with a HoloLens.</span></span> <span data-ttu-id="99f3f-146">H.</span><span class="sxs-lookup"><span data-stu-id="99f3f-146">H.</span></span> <span data-ttu-id="99f3f-147">Zhan, Y. Pekelny, O. Ulusoy.</span><span class="sxs-lookup"><span data-stu-id="99f3f-147">Zhan, Y. Pekelny, O. Ulusoy.</span></span>
* <span data-ttu-id="99f3f-148">17:15-17:30: Osservazioni finali.</span><span class="sxs-lookup"><span data-stu-id="99f3f-148">17:15-17:30: Final Remarks.</span></span>
