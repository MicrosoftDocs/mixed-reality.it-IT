---
title: Sguardo attento
description: HoloLens 2 consente un nuovo livello di contesto e comprensione umana all'interno dell'esperienza olografica, offrendo agli sviluppatori la possibilità di usare le informazioni relative agli utenti che stanno esaminando.
author: sostel
ms.author: sostel
ms.date: 04/05/2019
ms.topic: article
keywords: Rilevamento degli occhi, realtà mista, input, sguardo oculare, sguardi occhi
ms.openlocfilehash: 6c51e1cdc2057142f47b6f96e8a1f1aec0bbcc17
ms.sourcegitcommit: 3b32339c5d5c79eaecd84ed27254a8f4321731f1
ms.translationtype: MT
ms.contentlocale: it-IT
ms.lasthandoff: 08/27/2019
ms.locfileid: "70047100"
---
# <a name="eye-gaze-on-hololens-2"></a><span data-ttu-id="754f4-104">Eye-sguardi su HoloLens 2</span><span class="sxs-lookup"><span data-stu-id="754f4-104">Eye-gaze on HoloLens 2</span></span>
<span data-ttu-id="754f4-105">HoloLens 2 consente un nuovo livello di contesto e comprensione umana all'interno dell'esperienza olografica, offrendo agli sviluppatori la possibilità di usare le informazioni relative agli utenti che stanno esaminando.</span><span class="sxs-lookup"><span data-stu-id="754f4-105">HoloLens 2 allows for a new level of context and human understanding within the holographic experience by providing developers with the ability of using information about what users are looking at.</span></span> <span data-ttu-id="754f4-106">Questa pagina indica agli sviluppatori come possono trarre vantaggio dal monitoraggio degli sguardi per diversi casi d'uso, nonché da cosa cercare quando si progettano interfacce utente basate sull'occhio.</span><span class="sxs-lookup"><span data-stu-id="754f4-106">This page tells developers how they can benefit from eye tracking for various use cases as well as what to look for when designing eye-gaze-based user interfaces.</span></span> 


## <a name="device-support"></a><span data-ttu-id="754f4-107">Supporto di dispositivi</span><span class="sxs-lookup"><span data-stu-id="754f4-107">Device support</span></span>

<table>
<colgroup>
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
</colgroup>
<tr>
     <td><span data-ttu-id="754f4-108"><strong>Funzionalità</strong></span><span class="sxs-lookup"><span data-stu-id="754f4-108"><strong>Feature</strong></span></span></td>
     <td><span data-ttu-id="754f4-109"><a href="hololens-hardware-details.md"><strong>HoloLens (prima generazione)</strong></a></span><span class="sxs-lookup"><span data-stu-id="754f4-109"><a href="hololens-hardware-details.md"><strong>HoloLens (1st gen)</strong></a></span></span></td>
     <td><span data-ttu-id="754f4-110"><strong>HoloLens 2</strong></span><span class="sxs-lookup"><span data-stu-id="754f4-110"><strong>HoloLens 2</strong></span></span></td>
     <td><span data-ttu-id="754f4-111"><a href="immersive-headset-hardware-details.md"><strong>Visori VR immersive</strong></a></span><span class="sxs-lookup"><span data-stu-id="754f4-111"><a href="immersive-headset-hardware-details.md"><strong>Immersive headsets</strong></a></span></span></td>
</tr>
<tr>
     <td><span data-ttu-id="754f4-112">Sguardo attento</span><span class="sxs-lookup"><span data-stu-id="754f4-112">Eye-gaze</span></span></td>
     <td><span data-ttu-id="754f4-113">❌</span><span class="sxs-lookup"><span data-stu-id="754f4-113">❌</span></span></td>
     <td><span data-ttu-id="754f4-114">✔️</span><span class="sxs-lookup"><span data-stu-id="754f4-114">✔️</span></span></td>
     <td><span data-ttu-id="754f4-115">❌</span><span class="sxs-lookup"><span data-stu-id="754f4-115">❌</span></span></td>
</tr>
</table>

## <a name="use-cases"></a><span data-ttu-id="754f4-116">Casi d'uso</span><span class="sxs-lookup"><span data-stu-id="754f4-116">Use cases</span></span>
<span data-ttu-id="754f4-117">Il tracciamento oculare consente alle applicazioni di tenere traccia di dove guarda l'utente in tempo reale.</span><span class="sxs-lookup"><span data-stu-id="754f4-117">Eye tracking enables applications to track where the user is looking in real time.</span></span> <span data-ttu-id="754f4-118">I casi d'uso seguenti descrivono alcune interazioni possibili con la verifica degli occhi in realtà mista.</span><span class="sxs-lookup"><span data-stu-id="754f4-118">The following use cases describe some interactions that are possible with eye tracking in mixed reality.</span></span>
<span data-ttu-id="754f4-119">Tenere presente che il Toolkit per la [realtà mista](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html) è utile per fornire alcuni esempi interessanti e potenti per l'uso degli occhi, ad esempio selezioni di destinazioni supportate dagli occhi, rapide e senza problemi, nonché lo scorrimento automatico del testo in base a elementi esaminati dall'utente.</span><span class="sxs-lookup"><span data-stu-id="754f4-119">Keep in mind that the [Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html) is useful for providing several interesting and powerful examples for using eye tracking, such as quick and effortless eye-supported target selections as well as automatically scrolling through text based on what the user looks at.</span></span> 

### <a name="user-intent"></a><span data-ttu-id="754f4-120">Intenzione dell'utente</span><span class="sxs-lookup"><span data-stu-id="754f4-120">User intent</span></span>    
<span data-ttu-id="754f4-121">Le informazioni su dove e cosa esamina un utente forniscono un **contesto potente per altri input**, ad esempio Voice, Hands e Controllers.</span><span class="sxs-lookup"><span data-stu-id="754f4-121">Information about where and what a user looks at provides a powerful **context for other inputs**, such as voice, hands and controllers.</span></span>
<span data-ttu-id="754f4-122">Tali informazioni possono essere usate per diverse attività.</span><span class="sxs-lookup"><span data-stu-id="754f4-122">This can be used for various tasks.</span></span>
<span data-ttu-id="754f4-123">Questo, ad esempio, può variare da un punto di vista rapido e semplice a quello della scena, semplicemente esaminando un ologramma e affermando "Select" (vedere anche lo [sguardo e il commit](gaze-and-commit.md)) o dicendo "put this...", quindi esaminando il percorso in cui l'utente desidera inserire l'ologramma e pronunciare "... ".</span><span class="sxs-lookup"><span data-stu-id="754f4-123">For example, this can range from quickly and effortlessly **targeting** across the scene by simply looking at a hologram and saying "select" (also see [Head-gaze and commit](gaze-and-commit.md)) or by saying "put this...", then looking over to where the user wants to place the hologram and say "...there".</span></span> <span data-ttu-id="754f4-124">Per alcuni esempi, vedi [Mixed Reality Toolkit - Selezione della destinazione con lo sguardo](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_TargetSelection.html) e [Mixed Reality Toolkit - Posizionamento della destinazione con lo sguardo](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Positioning.html).</span><span class="sxs-lookup"><span data-stu-id="754f4-124">Examples for this can be found in [Mixed Reality Toolkit - Eye-supported Target Selection](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_TargetSelection.html) and [Mixed Reality Toolkit - Eye-supported Target Positioning](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Positioning.html).</span></span>

<span data-ttu-id="754f4-125">Inoltre, un esempio per finalità utente potrebbe includere l'uso di informazioni sugli elementi esaminati dagli utenti per migliorare l'engagement con agenti virtuali incorporati e ologrammi interattivi.</span><span class="sxs-lookup"><span data-stu-id="754f4-125">Additionally, an example for user intent might include using information about what users look at to enhance engagement with embodied virtual agents and interactive holograms.</span></span> <span data-ttu-id="754f4-126">Ad esempio, gli agenti virtuali potrebbero adattare le opzioni disponibili e il relativo comportamento in base al contenuto attualmente visualizzato.</span><span class="sxs-lookup"><span data-stu-id="754f4-126">For instance, virtual agents might adapt available options and their behavior based on currently viewed content.</span></span> 

### <a name="implicit-actions"></a><span data-ttu-id="754f4-127">Azioni implicite</span><span class="sxs-lookup"><span data-stu-id="754f4-127">Implicit actions</span></span>
<span data-ttu-id="754f4-128">La categoria delle azioni implicite è strettamente correlata all'intenzione dell'utente.</span><span class="sxs-lookup"><span data-stu-id="754f4-128">The category of implicit actions closely relates to user intent.</span></span>
<span data-ttu-id="754f4-129">L'idea è che gli ologrammi o gli elementi dell'interfaccia utente reagiscono in modo alquanto istintivo che potrebbe non sembrare che l'utente interagisca con il sistema, ma piuttosto che il sistema e l'utente siano sincronizzati. Un esempio è lo **scorrimento automatico basato sull'occhio** , in cui l'utente legge il testo mentre il testo continua a scorrere o a scorrere sincronizzato con lo sguardo dell'utente.</span><span class="sxs-lookup"><span data-stu-id="754f4-129">The idea is that holograms or user interface elements react in a somewhat instinctual way that may not even feel like the user is interacting with the system at all, but rather that the system and the user are in sync. One example is **eye-gaze-based auto scroll** where the user reads text as the text continues to scroll or flow in sync with the user's gaze.</span></span> <span data-ttu-id="754f4-130">Un aspetto fondamentale è che la velocità di scorrimento si adatta alla velocità di lettura dell'utente.</span><span class="sxs-lookup"><span data-stu-id="754f4-130">A key aspect of this is that scrolling speed adapts to the reading speed of the user.</span></span>
<span data-ttu-id="754f4-131">Un altro esempio è **lo zoom e la panoramica supportati dagli occhi,** in cui l'utente può avere la tendenza a concentrarsi esattamente su quello che si sta concentrando.</span><span class="sxs-lookup"><span data-stu-id="754f4-131">Another example is **eye-supported zoom and pan** where the user can feel like diving exactly toward what he or she is focused on.</span></span> <span data-ttu-id="754f4-132">L'attivazione dello zoom e il controllo della velocità di zoom possono essere controllati da input voce o mano, che è importante per fornire all'utente la sensazione di controllo, evitando così la sovraccarica.</span><span class="sxs-lookup"><span data-stu-id="754f4-132">Triggering zoom and controlling zoom speed can be controlled by voice or hand input, which is important for providing the user with the feeling of control while avoiding being overwhelmed.</span></span> <span data-ttu-id="754f4-133">Di seguito vengono descritte in dettaglio le linee guida di progettazione.</span><span class="sxs-lookup"><span data-stu-id="754f4-133">We will talk about these design guidelines in more detail below.</span></span> <span data-ttu-id="754f4-134">Una volta eseguito lo zoom avanti, l'utente può seguire in modo semplice, ad esempio, il corso di una strada per esplorare il suo quartiere utilizzando semplicemente il proprio sguardo.</span><span class="sxs-lookup"><span data-stu-id="754f4-134">Once zoomed in, the user can  smoothly follow, for example, the course of a street to explore his or her neighborhood by simply using their eye-gaze.</span></span>
<span data-ttu-id="754f4-135">Alcune demo di esempio per questi tipi di interazioni sono disponibili in [Mixed Reality Toolkit - Navigazione con gli occhi](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Navigation.html).</span><span class="sxs-lookup"><span data-stu-id="754f4-135">Demo examples for these types of interactions can be found in the [Mixed Reality Toolkit - Eye-supported Navigation](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Navigation.html) sample.</span></span>

<span data-ttu-id="754f4-136">Altri casi d'uso per le _azioni implicite_ possono includere:</span><span class="sxs-lookup"><span data-stu-id="754f4-136">Additional use cases for _implicit actions_ can include:</span></span>
- <span data-ttu-id="754f4-137">**Notifiche intelligenti:** non vieni mai disturbato dalle notifiche che vengono visualizzate proprio nel punto su cui ti sei soffermato?</span><span class="sxs-lookup"><span data-stu-id="754f4-137">**Smart notifications:** Ever get annoyed by notifications popping up right where you were focusing?</span></span> <span data-ttu-id="754f4-138">Prendendo in considerazione le informazioni a cui un utente presta attenzione, è possibile migliorare questa esperienza compensando le notifiche da cui l'utente sta attualmente guardando.</span><span class="sxs-lookup"><span data-stu-id="754f4-138">Taking into account what a user is paying attention to, you can make this experience better by offsetting notifications from where the user is currently gazing.</span></span> <span data-ttu-id="754f4-139">Questo limita le distrazioni e le chiude automaticamente dopo che l'utente ha terminato la lettura.</span><span class="sxs-lookup"><span data-stu-id="754f4-139">This limits distractions and automatically dismisses them once the user is finished reading.</span></span> 
- <span data-ttu-id="754f4-140">**Ologrammi reattivi:** Olografici che reagiscono in maniera impercettibile quando si osservano.</span><span class="sxs-lookup"><span data-stu-id="754f4-140">**Attentive holograms:** Holograms that subtly react when being gazed upon.</span></span> <span data-ttu-id="754f4-141">Questo può variare da elementi dell'interfaccia utente leggermente luminosi a un fiore lentamente fiorito a un animale domestico virtuale che inizia a esaminare l'utente o a provare a evitare gli sguardi degli utenti dopo un prolungamento.</span><span class="sxs-lookup"><span data-stu-id="754f4-141">This can range from slightly glowing UI elements to a slowly blooming flower to a virtual pet starting to look back at the user or trying to avoid the user's eye-gaze after a prolonged stare.</span></span> <span data-ttu-id="754f4-142">Questa interazione potrebbe offrire un'interessante sensazione di connettività e soddisfazione nell'applicazione.</span><span class="sxs-lookup"><span data-stu-id="754f4-142">This interaction might provide an interesting sense of connectivity and satisfaction in your application.</span></span>

### <a name="attention-tracking"></a><span data-ttu-id="754f4-143">Tracciamento dell'attenzione</span><span class="sxs-lookup"><span data-stu-id="754f4-143">Attention tracking</span></span>   
<span data-ttu-id="754f4-144">Le informazioni su dove o quali utenti esaminano sono uno strumento estremamente potente per valutare l'usabilità delle progettazioni e per identificare i problemi nei flussi di lavoro efficienti.</span><span class="sxs-lookup"><span data-stu-id="754f4-144">Information about where or what users look at is an immensely powerful tool to assess usability of designs, and to identify problems in efficient workflows.</span></span> <span data-ttu-id="754f4-145">La visualizzazione e l'analisi dei tracciati degli occhi sono una pratica comune in varie aree di applicazione.</span><span class="sxs-lookup"><span data-stu-id="754f4-145">Eye tracking visualization and analytics are a common practice in various application areas.</span></span> <span data-ttu-id="754f4-146">Con HoloLens 2, viene fornita una nuova dimensione a questa comprensione perché gli ologrammi 3D possono essere inseriti in contesti reali e valutati di conseguenza.</span><span class="sxs-lookup"><span data-stu-id="754f4-146">With HoloLens 2, we provide a new dimension to this understanding as 3D holograms can be placed in real-world contexts and assessed accordingly.</span></span> <span data-ttu-id="754f4-147">Il [Toolkit di realtà mista](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html) fornisce esempi di base per la registrazione e il caricamento dei dati di rilevamento degli occhi e come visualizzarli.</span><span class="sxs-lookup"><span data-stu-id="754f4-147">The [Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html) provides basic examples for logging and loading eye tracking data and  how to visualize them.</span></span>

<span data-ttu-id="754f4-148">Altre applicazioni in quest'area possono includere:</span><span class="sxs-lookup"><span data-stu-id="754f4-148">Other applications in this area can include:</span></span> 
-   <span data-ttu-id="754f4-149">**Visualizzazione sguardo remoto:** Consente di visualizzare i collaboratori remoti che esaminano per verificare se le istruzioni sono state comprese correttamente e seguite.</span><span class="sxs-lookup"><span data-stu-id="754f4-149">**Remote eye-gaze visualization:** Visualize what remote collaborators are looking at to ensure whether instructions are correctly understood and followed.</span></span>
-   <span data-ttu-id="754f4-150">**Studi di ricerca sugli utenti:** Il rilevamento dell'attenzione può essere usato per esplorare il modo in cui gli utenti di esperti e esperti analizzano visivamente il contenuto o il modo in cui coordinano le attività complesse, ad esempio per l'analisi di dati medici o di macchinari operativi.</span><span class="sxs-lookup"><span data-stu-id="754f4-150">**User research studies:** Attention tracking can be used to explore the way novice vs. expert users visually analyze content or how their hand-eye-coordination for complex tasks, such as for analysis of medical data or while operating machinery.</span></span>
-   <span data-ttu-id="754f4-151">**Simulazioni di formazione e monitoraggio delle prestazioni:** è possibile fare pratica e ottimizzare l'esecuzione delle attività identificando i colli di bottiglia in modo più efficace nel flusso di esecuzione.</span><span class="sxs-lookup"><span data-stu-id="754f4-151">**Training simulations and Performance monitoring:** Practice and optimize the execution of tasks by identifying bottlenecks more effectively in the execution flow.</span></span>
-   <span data-ttu-id="754f4-152">**Valutazioni delle progettazioni e ricerche pubblicitarie e di marketing:** Il rilevamento degli occhi è uno strumento comune per la ricerca di mercato quando si valutano i siti Web e i progetti di prodotti.</span><span class="sxs-lookup"><span data-stu-id="754f4-152">**Design evaluations, advertisement and marketing research:** Eye tracking is a common tool for market research when evaluating website and product designs.</span></span>

### <a name="additional-use-cases"></a><span data-ttu-id="754f4-153">Altri casi di utilizzo</span><span class="sxs-lookup"><span data-stu-id="754f4-153">Additional use cases</span></span>
- <span data-ttu-id="754f4-154">**Giochi:** hai mai sognato di avere dei superpoteri?</span><span class="sxs-lookup"><span data-stu-id="754f4-154">**Gaming:** Ever wanted to have superpowers?</span></span> <span data-ttu-id="754f4-155">Ora hai questa opportunità.</span><span class="sxs-lookup"><span data-stu-id="754f4-155">Here's your chance!</span></span> <span data-ttu-id="754f4-156">È possibile levitare gli ologrammi guardandoli.</span><span class="sxs-lookup"><span data-stu-id="754f4-156">You can levitate holograms by staring at them.</span></span> <span data-ttu-id="754f4-157">lanciare raggi laser dagli occhi,</span><span class="sxs-lookup"><span data-stu-id="754f4-157">Shoot laser beams from your eyes.</span></span> <span data-ttu-id="754f4-158">Trasformare i nemici in pietra o bloccarli.</span><span class="sxs-lookup"><span data-stu-id="754f4-158">Turn enemies into stone or freeze them.</span></span> <span data-ttu-id="754f4-159">oppure usare la tua vista a raggi x per esplorare l'interno degli edifici.</span><span class="sxs-lookup"><span data-stu-id="754f4-159">Use your x-ray vision to explore buildings.</span></span> <span data-ttu-id="754f4-160">Insomma, con la tua immaginazione potrai superare ogni ostacolo.</span><span class="sxs-lookup"><span data-stu-id="754f4-160">Your imagination is the limit!</span></span>  

- <span data-ttu-id="754f4-161">**Avatar espressivi:** Il rilevamento degli occhi negli Avatar 3D più espressivi usa i dati di tracking degli occhi dinamici per animare gli occhi dell'avatar che indicano l'aspetto dell'utente.</span><span class="sxs-lookup"><span data-stu-id="754f4-161">**Expressive avatars:** Eye tracking aids in more expressive 3D avatars by using live eye tracking data to animate the avatar's eyes that indicate what the user is looking at.</span></span> <span data-ttu-id="754f4-162">Aggiunge anche una maggiore espressività aggiungendo i lampeggi.</span><span class="sxs-lookup"><span data-stu-id="754f4-162">It also adds more expressiveness by adding blinks.</span></span> 

- <span data-ttu-id="754f4-163">**Immissione di testo:** Il rilevamento degli occhi può essere usato come alternativa per la voce di testo a basso sforzo, soprattutto quando il discorso o le mani non sono convenienti da usare.</span><span class="sxs-lookup"><span data-stu-id="754f4-163">**Text entry:** Eye tracking can be used as an alternative for low-effort text entry, especially when speech or hands are inconvenient to use.</span></span> 


## <a name="eye-tracking-api"></a><span data-ttu-id="754f4-164">API di tracciamento oculare</span><span class="sxs-lookup"><span data-stu-id="754f4-164">Eye tracking API</span></span>
<span data-ttu-id="754f4-165">Prima di approfondire le linee guida di progettazione specifiche per l'interazione con gli occhi, è opportuno evidenziare brevemente le funzionalità offerte dall'API HoloLens 2 eye tracker per gli sviluppatori.</span><span class="sxs-lookup"><span data-stu-id="754f4-165">Before going into detail about the specific design guidelines for eye-gaze interaction, we want to briefly point out the capabilities that the HoloLens 2 Eye Tracker API provides to developers.</span></span> <span data-ttu-id="754f4-166">Fornisce una singola occhiata, che indica l'origine e la direzione, fornendo dati a circa _30 Hz_.</span><span class="sxs-lookup"><span data-stu-id="754f4-166">It provides a single eye-gaze--gaze origin and direction--providing data at approximately _30 Hz_.</span></span> 

<span data-ttu-id="754f4-167">Lo sguardo stimato si trova all'interno della CA.</span><span class="sxs-lookup"><span data-stu-id="754f4-167">The predicted eye-gaze lies within ca.</span></span> <span data-ttu-id="754f4-168">1,0-1,5 gradi nell'angolo visivo intorno alla destinazione effettiva.</span><span class="sxs-lookup"><span data-stu-id="754f4-168">1.0 - 1.5 degrees in visual angle around the actual target.</span></span> <span data-ttu-id="754f4-169">Poiché sono prevedibili lievi imprecisioni, effettua la pianificazione in modo da lasciare un po' di margine per questo valore limite minimo.</span><span class="sxs-lookup"><span data-stu-id="754f4-169">As slight imprecisions are expected, you should plan for some margin around this lower bound value.</span></span> <span data-ttu-id="754f4-170">Più avanti verranno forniti altri dettagli in merito.</span><span class="sxs-lookup"><span data-stu-id="754f4-170">We will discuss this more below.</span></span> <span data-ttu-id="754f4-171">Per un accurato funzionamento del tracciamento oculare, ogni utente deve essere sottoposto a un'apposita calibrazione.</span><span class="sxs-lookup"><span data-stu-id="754f4-171">For eye tracking to work accurately, each user is required to go through an eye tracking user calibration.</span></span> 

<span data-ttu-id="754f4-172">![Dimensioni ottimali della destinazione a una distanza di 2 metri](images/gazetargeting-size-1000px.jpg)</span><span class="sxs-lookup"><span data-stu-id="754f4-172">![Optimal target size at 2 meter distance](images/gazetargeting-size-1000px.jpg)</span></span><br>
<span data-ttu-id="754f4-173">*Dimensioni di destinazione ottimali a distanza di 2 metri*</span><span class="sxs-lookup"><span data-stu-id="754f4-173">*Optimal target size at a 2-meter distance*</span></span>
<br>
<br>
<span data-ttu-id="754f4-174">L' [API di rilevamento degli occhi](https://docs.microsoft.com/en-us/uwp/api/windows.perception.people.eyespose) è accessibile tramite:' Windows. Perception. people. EyesPose '.</span><span class="sxs-lookup"><span data-stu-id="754f4-174">The [Eye Tracking API](https://docs.microsoft.com/en-us/uwp/api/windows.perception.people.eyespose) is accessible through: \`Windows.Perception.People.EyesPose'.</span></span> 

## <a name="calibration"></a><span data-ttu-id="754f4-175">Calibrazione</span><span class="sxs-lookup"><span data-stu-id="754f4-175">Calibration</span></span> 
<span data-ttu-id="754f4-176">Per un accurato funzionamento del tracciamento oculare, ogni utente deve essere sottoposto a un'apposita calibrazione.</span><span class="sxs-lookup"><span data-stu-id="754f4-176">For eye tracking to work accurately, each user is required to go through an eye tracking user calibration.</span></span> <span data-ttu-id="754f4-177">In HoloLens 2 all'utente viene richiesto di calibrare gli oggetti visivi durante la configurazione del dispositivo, esaminando il set di destinazioni di fissaggio.</span><span class="sxs-lookup"><span data-stu-id="754f4-177">On HoloLens 2, the user is prompted to calibrate visuals during device setup, by looking at the set of fixation targets.</span></span> <span data-ttu-id="754f4-178">In questo modo, il dispositivo può modificare il dispositivo per un'esperienza di visualizzazione confortevole e qualitativa per l'utente e assicurare un rilevamento accurato degli occhi allo stesso tempo.</span><span class="sxs-lookup"><span data-stu-id="754f4-178">This allows the device to adjust the device for a comfortable and quality viewing experience for the user and ensure accurate eye tracking at the same time.</span></span>  <span data-ttu-id="754f4-179">La calibrazione dovrebbe funzionare per la maggior parte degli utenti, ma in alcuni casi l'utente potrebbe non essere in grado di calibrare correttamente.</span><span class="sxs-lookup"><span data-stu-id="754f4-179">Calibration should work for most of the users, but there are cases in which user might be unable to calibrate successfully.</span></span>  <span data-ttu-id="754f4-180">Per ulteriori informazioni sulla calibrazione, controllare la [calibrazione](https://docs.microsoft.com/en-us/windows/mixed-reality/calibration).</span><span class="sxs-lookup"><span data-stu-id="754f4-180">To learn more about the calibration, please check [Calibration](https://docs.microsoft.com/en-us/windows/mixed-reality/calibration).</span></span>

## <a name="eye-gaze-design-guidelines"></a><span data-ttu-id="754f4-181">Linee guida per la progettazione degli sguardi</span><span class="sxs-lookup"><span data-stu-id="754f4-181">Eye-gaze design guidelines</span></span>
<span data-ttu-id="754f4-182">La creazione di un'interazione che sfrutta la scelta rapida per gli occhi in rapida evoluzione può risultare complessa.</span><span class="sxs-lookup"><span data-stu-id="754f4-182">Building an interaction that takes advantage of fast-moving eye targeting can be challenging.</span></span> <span data-ttu-id="754f4-183">In questa sezione vengono riepilogati i vantaggi e le questioni principali da considerare durante la progettazione dell'applicazione.</span><span class="sxs-lookup"><span data-stu-id="754f4-183">In this section, we summarize the key advantages and challenges to consider when designing your application.</span></span> 

### <a name="benefits-of-eye-gaze-input"></a><span data-ttu-id="754f4-184">Vantaggi dell'input con sguardo a occhio</span><span class="sxs-lookup"><span data-stu-id="754f4-184">Benefits of eye-gaze input</span></span>
- <span data-ttu-id="754f4-185">**Puntamento ad alta velocità.**</span><span class="sxs-lookup"><span data-stu-id="754f4-185">**High speed pointing.**</span></span> <span data-ttu-id="754f4-186">Il muscolo occhi è il muscolo più veloce nel corpo umano.</span><span class="sxs-lookup"><span data-stu-id="754f4-186">The eye muscle is the fastest reacting muscle in the human body.</span></span> 

- <span data-ttu-id="754f4-187">**Sforzo limitato.**</span><span class="sxs-lookup"><span data-stu-id="754f4-187">**Low effort.**</span></span> <span data-ttu-id="754f4-188">Non sono quasi necessari movimenti fisici.</span><span class="sxs-lookup"><span data-stu-id="754f4-188">Barely any physical movements are necessary.</span></span> 

- <span data-ttu-id="754f4-189">**Natura implicita.**</span><span class="sxs-lookup"><span data-stu-id="754f4-189">**Implicitness.**</span></span> <span data-ttu-id="754f4-190">Spesso descritta dagli utenti come "lettura mentale", le informazioni sui movimenti degli occhi di un utente consentono al sistema di sapere quale destinazione prevede l'intervento dell'utente.</span><span class="sxs-lookup"><span data-stu-id="754f4-190">Often described by users as "mind reading", information about a user's eye movements lets the system know which target the user plans to engage.</span></span> 

- <span data-ttu-id="754f4-191">**Canale di input alternativo.**</span><span class="sxs-lookup"><span data-stu-id="754f4-191">**Alternative input channel.**</span></span> <span data-ttu-id="754f4-192">Eye-sguardi può offrire un potente input di supporto per l'input vocale e di mano che si basa su anni di esperienza degli utenti in base al coordinamento degli occhi.</span><span class="sxs-lookup"><span data-stu-id="754f4-192">Eye-gaze can provide a powerful supporting input for hand and voice input building on years of experience from users based on their hand-eye coordination.</span></span>

- <span data-ttu-id="754f4-193">**Attenzione visiva.**</span><span class="sxs-lookup"><span data-stu-id="754f4-193">**Visual attention.**</span></span> <span data-ttu-id="754f4-194">Un altro vantaggio importante è la possibilità di dedurre ciò a cui un utente presta attenzione.</span><span class="sxs-lookup"><span data-stu-id="754f4-194">Another important benefit is the possibility to infer what a user is paying attention to.</span></span> <span data-ttu-id="754f4-195">Questo può essere utile in diverse aree applicative, da valutare in modo più efficace le diverse progettazioni per aiutare le interfacce utente più intelligenti e suggerimenti sociali avanzati per la comunicazione remota.</span><span class="sxs-lookup"><span data-stu-id="754f4-195">This can help in various application areas ranging from more effectively evaluating different designs to aiding in smarter user interfaces and enhanced social cues for remote communication.</span></span>

<span data-ttu-id="754f4-196">In breve, l'uso di Eye-sguardi come input offre un segnale contestuale rapido e facile.</span><span class="sxs-lookup"><span data-stu-id="754f4-196">In a nutshell, using eye-gaze as an input offers a fast and effortless contextual signal.</span></span> <span data-ttu-id="754f4-197">Questa operazione è particolarmente efficace se combinata con altri input, ad esempio input *vocale* e *manuale* , per confermare la finalità dell'utente.</span><span class="sxs-lookup"><span data-stu-id="754f4-197">This is particularly powerful when combined with other inputs such as *voice* and *manual* input to confirm the user's intent.</span></span>


### <a name="challenges-of-eye-gaze-as-an-input"></a><span data-ttu-id="754f4-198">Problemi di Eye-sguardi come input</span><span class="sxs-lookup"><span data-stu-id="754f4-198">Challenges of eye-gaze as an input</span></span>
<span data-ttu-id="754f4-199">Grazie a un elevato consumo di energia, è molto responsabile.</span><span class="sxs-lookup"><span data-stu-id="754f4-199">With lots of power, comes lots of responsibility.</span></span>
<span data-ttu-id="754f4-200">Sebbene possa essere usato per creare esperienze utente soddisfacenti, in modo da essere un supereroe, è anche importante sapere cosa non è utile per tenere conto di questo aspetto.</span><span class="sxs-lookup"><span data-stu-id="754f4-200">While eye-gaze can be used to create satisfying user experiences that makes you feel like a superhero, it is also important to know what it is not good at to appropriately account for this.</span></span> <span data-ttu-id="754f4-201">Di seguito sono illustrate alcune delle *questioni* da prendere in considerazione e il modo in cui risolverle quando si lavora con l'input Eye-sguardi:</span><span class="sxs-lookup"><span data-stu-id="754f4-201">The following discusses some *challenges* to consider as well as how to address them when working with eye-gaze input:</span></span> 

- <span data-ttu-id="754f4-202">**Il controllo degli sguardi è "always on"** Nel momento in cui si aprono i coperchi degli occhi, gli occhi iniziano a fissando su elementi nell'ambiente.</span><span class="sxs-lookup"><span data-stu-id="754f4-202">**Your eye-gaze is "always on"** The moment you open your eye lids, your eyes start fixating on things in the environment.</span></span> <span data-ttu-id="754f4-203">Reagire a ogni aspetto effettuato e rilasciare accidentalmente le azioni perché si è verificato un problema per troppo tempo, si otterrebbe un'esperienza insoddisfacente.</span><span class="sxs-lookup"><span data-stu-id="754f4-203">Reacting to every look you make and accidentally issuing actions because you looked at something for too long would result in an unsatisfying experience.</span></span>
<span data-ttu-id="754f4-204">È quindi consigliabile combinare gli occhi con un *comando vocale*, un *gesto della mano*, un *clic del pulsante* o un'abitazione estesa per attivare la selezione di una destinazione.</span><span class="sxs-lookup"><span data-stu-id="754f4-204">Therefore we recommend combining eye-gaze with a *voice command*, *hand gesture*, *button click* or extended dwell to trigger the selection of a target.</span></span>
<span data-ttu-id="754f4-205">Questa soluzione consente anche una modalità in cui l'utente può esaminarsi liberamente senza essere sopraffatto dall'attivazione involontaria di qualcosa.</span><span class="sxs-lookup"><span data-stu-id="754f4-205">This solution also allows for a mode in which the user can freely look around without being overwhelmed by involuntarily triggering something.</span></span> <span data-ttu-id="754f4-206">Questo problema deve anche essere preso in considerazione quando si progettano commenti visivi e uditivi quando si esamina semplicemente una destinazione.</span><span class="sxs-lookup"><span data-stu-id="754f4-206">This issue should also be considered when designing visual and auditory feedback when merely looking at a target.</span></span>
<span data-ttu-id="754f4-207">Non bombardare l'utente con effetti immediati e improvvisi o suoni al passaggio del puntatore.</span><span class="sxs-lookup"><span data-stu-id="754f4-207">Do not overwhelm the user with immediate pop-out effects or hover sounds.</span></span> <span data-ttu-id="754f4-208">La sottigliezza è Key.</span><span class="sxs-lookup"><span data-stu-id="754f4-208">Subtlety is key.</span></span> <span data-ttu-id="754f4-209">Nella sezione relativa ai suggerimenti per la progettazione verranno illustrate alcune procedure consigliate su questo argomento.</span><span class="sxs-lookup"><span data-stu-id="754f4-209">We will discuss some best practices for this further below when talking about design recommendations.</span></span>

- <span data-ttu-id="754f4-210">**Confronto tra osservazione e controllo** Si supponga di voler raddrizzare con precisione una fotografia sulla parete.</span><span class="sxs-lookup"><span data-stu-id="754f4-210">**Observation vs. control** Imagine that you want to precisely straighten a photograph on your wall.</span></span> <span data-ttu-id="754f4-211">Guardi pertanto i bordi e l'area circostante per verificare che l'immagine sia allineata correttamente.</span><span class="sxs-lookup"><span data-stu-id="754f4-211">You look at its borders and its surroundings to see if it aligns well.</span></span> <span data-ttu-id="754f4-212">A questo punto, si supponga di voler usare il proprio sguardo come input per spostare l'immagine.</span><span class="sxs-lookup"><span data-stu-id="754f4-212">Now imagine how you would do that when you want to use your eye-gaze as an input to move the picture.</span></span> <span data-ttu-id="754f4-213">L'esecuzione diventa veramente difficoltosa.</span><span class="sxs-lookup"><span data-stu-id="754f4-213">Difficult, isn't it?</span></span> <span data-ttu-id="754f4-214">Viene descritto il doppio ruolo di occhio quando è necessario sia per l'input che per il controllo.</span><span class="sxs-lookup"><span data-stu-id="754f4-214">This describes the double role of eye-gaze when it is required both for input and control.</span></span> 

- <span data-ttu-id="754f4-215">**Uscita prima del clic:** Per le selezioni di destinazione rapide, la ricerca ha dimostrato che lo sguardo d'occhio di un utente può andare avanti prima di concludere un clic manuale (ad esempio, AirTap).</span><span class="sxs-lookup"><span data-stu-id="754f4-215">**Leave before click:** For quick target selections, research has shown that a user's eye-gaze can move on before concluding a manual click (e.g., an airtap).</span></span> <span data-ttu-id="754f4-216">Di conseguenza, è necessario prestare particolare attenzione alla sincronizzazione del segnale di sguardo rapido con un input di controllo più lento (ad esempio, Voice, Hands, controller).</span><span class="sxs-lookup"><span data-stu-id="754f4-216">Hence, special attention must be paid to synchronizing the fast eye-gaze signal with slower control input (e.g., voice, hands, controller).</span></span>

- <span data-ttu-id="754f4-217">**Destinazioni di piccole dimensioni:** Si conosce la sensazione quando si tenta di leggere il testo che è troppo piccolo per leggere comodamente?</span><span class="sxs-lookup"><span data-stu-id="754f4-217">**Small targets:** Do you know the feeling when you try to read text that is just a bit too small to read comfortably?</span></span> <span data-ttu-id="754f4-218">Questa sensazione di limitazione degli sguardi può causare la disattivazione e la disattivazione, perché si tenta di riadattare gli occhi per concentrarsi meglio.</span><span class="sxs-lookup"><span data-stu-id="754f4-218">This straining feeling on your eyes can cause you to feel tired and worn out because you try to readjust your eyes to focus better.</span></span>
<span data-ttu-id="754f4-219">Si tratta di un sentimento che è possibile richiamare negli utenti quando si impone loro di selezionare destinazioni troppo piccole nell'applicazione usando la destinazione degli occhi.</span><span class="sxs-lookup"><span data-stu-id="754f4-219">This is a feeling you might invoke in your users when forcing them to select targets that are too small in your application using eye targeting.</span></span>
<span data-ttu-id="754f4-220">Durante la progettazione, per creare un'esperienza piacevole e confortevole per gli utenti, è consigliabile definire destinazioni con un angolo visivo di almeno 2 gradi.</span><span class="sxs-lookup"><span data-stu-id="754f4-220">For your design, to create a pleasant and comfortable experience for your users, we recommend that targets should be at least 2° in visual angle, preferably larger.</span></span>

- <span data-ttu-id="754f4-221">**Spostamenti occhi** incompleti Gli occhi eseguono movimenti rapidi dalla fissa alla fissa.</span><span class="sxs-lookup"><span data-stu-id="754f4-221">**Ragged eye-gaze movements** Our eyes perform rapid movements from fixation to fixation.</span></span> <span data-ttu-id="754f4-222">Se guardi i percorsi di analisi dei movimenti oculari registrati, noterai che risultano irregolari.</span><span class="sxs-lookup"><span data-stu-id="754f4-222">If you look at scan paths of recorded eye movements, you can see that they look ragged.</span></span> <span data-ttu-id="754f4-223">Gli occhi si muovono velocemente e con salti spontanei rispetto al *puntamento con la testa* o ai *movimenti delle mani*.</span><span class="sxs-lookup"><span data-stu-id="754f4-223">Your eyes move quickly and in spontaneous jumps in comparison to *head gaze* or *hand motions*.</span></span>  

- <span data-ttu-id="754f4-224">**Affidabilità del tracciamento:** il tracciamento oculare può diventare leggermente meno affidabile con il mutare della luce, in quanto gli occhi devono adattarsi alle nuove condizioni di illuminazione.</span><span class="sxs-lookup"><span data-stu-id="754f4-224">**Tracking reliability:** Eye tracking accuracy may degrade a little in changing light as your eye adjust to the new conditions.</span></span>
<span data-ttu-id="754f4-225">Sebbene questo non abbia necessariamente effetto sulla progettazione dell'applicazione, in quanto l'accuratezza dovrebbe essere compresa nel limite di 2 °, potrebbe essere necessario che l'utente esegua di nuovo la calibrazione.</span><span class="sxs-lookup"><span data-stu-id="754f4-225">While this should not necessarily affect your application design, as the accuracy should be within the 2° limitation, i might be necessary for the user to calibrate again.</span></span> 


## <a name="design-recommendations"></a><span data-ttu-id="754f4-226">Suggerimenti per la progettazione</span><span class="sxs-lookup"><span data-stu-id="754f4-226">Design recommendations</span></span>
<span data-ttu-id="754f4-227">Di seguito è riportato un elenco di raccomandazioni di progettazione specifiche in base ai vantaggi e alle problemi descritti per l'input degli sguardi:</span><span class="sxs-lookup"><span data-stu-id="754f4-227">The following is a list of specific design recommendations based on the described advantages and challenges for eye-gaze input:</span></span>

1. <span data-ttu-id="754f4-228">**Il controllo degli sguardi non è uguale a quello della testa:**</span><span class="sxs-lookup"><span data-stu-id="754f4-228">**Eye-gaze is not the same as Head-gaze:**</span></span>
    - <span data-ttu-id="754f4-229">**Considerare se i movimenti dell'occhio, rapidi ma non uniformi, sono adatti per l'attività di input da svolgere:** Sebbene i nostri spostamenti rapidi e incompleti siano ottimi per selezionare rapidamente le destinazioni nel campo di visualizzazione, è meno applicabile per le attività che richiedono traiettorie di input uniformi, ad esempio la creazione o l'inclusione di annotazioni.</span><span class="sxs-lookup"><span data-stu-id="754f4-229">**Consider whether fast yet ragged eye movements fit your input task:** While our fast and ragged eye movements are great at quickly selecting targets across our field of view, it is less applicable for tasks that require smooth input trajectories (e.g., drawing or encircling annotations).</span></span> <span data-ttu-id="754f4-230">In questo caso, è preferibile usare il puntamento con la mano o con la testa.</span><span class="sxs-lookup"><span data-stu-id="754f4-230">In this case, hand or head pointing should be preferred.</span></span>
  
    - <span data-ttu-id="754f4-231">**Evitare di allungare direttamente gli sguardi degli utenti (ad esempio, un dispositivo di scorrimento o un cursore).**</span><span class="sxs-lookup"><span data-stu-id="754f4-231">**Avoid attaching something directly to the user’s eye-gaze (e.g., a slider or cursor).**</span></span>
<span data-ttu-id="754f4-232">Nel caso di un cursore, questo può comportare un effetto di "cursore in fuga" a causa di lievi offset nel segnale oculare proiettato.</span><span class="sxs-lookup"><span data-stu-id="754f4-232">In case of a cursor, this may result in the “fleeing cursor” effect due to slight offsets in the projected eye-gaze signal.</span></span> <span data-ttu-id="754f4-233">Nel caso di un dispositivo di scorrimento, può entrare in conflitto con il doppio ruolo del controllo del dispositivo di scorrimento con gli occhi, ma anche per verificare se l'oggetto si trova nella posizione corretta.</span><span class="sxs-lookup"><span data-stu-id="754f4-233">In case of a slider, it can conflict with the double role of controlling the slider with your eyes while also wanting to check whether the object is at the correct location.</span></span> <span data-ttu-id="754f4-234">In breve, gli utenti possono diventare sopraffatti e distratti, soprattutto se il segnale è impreciso per tale utente.</span><span class="sxs-lookup"><span data-stu-id="754f4-234">In a nutshell, users could become overwhelmed and distracted, especially if the signal is imprecise for that user.</span></span> 
  
2. <span data-ttu-id="754f4-235">**Combinare gli sguardi con gli altri input:** L'integrazione di Eye Tracking con altri input, ad esempio movimenti della mano, comandi vocali o pressioni dei pulsanti, offre diversi vantaggi:</span><span class="sxs-lookup"><span data-stu-id="754f4-235">**Combine eye-gaze with other inputs:** The integration of eye tracking with other inputs, such as hand gestures, voice commands or button presses, serves several advantages:</span></span>
    - <span data-ttu-id="754f4-236">**Possibilità di consentire la libera osservazione:** Dato che il ruolo principale degli occhi è osservare l'ambiente, è importante che gli utenti siano in grado di spostarsi senza attivare commenti o azioni (visivi, uditivi e così via).</span><span class="sxs-lookup"><span data-stu-id="754f4-236">**Allow for free observation:** Given that the main role of our eyes is to observe our environment, it is important users are allowed to look around without triggering any (visual, auditory, etc.) feedback or actions.</span></span> 
    <span data-ttu-id="754f4-237">La combinazione di rilevamento degli occhi con un altro controllo di input consente una transizione uniforme tra le modalità di osservazione e controllo di input.</span><span class="sxs-lookup"><span data-stu-id="754f4-237">Combining eye tracking with another input control allows smooth transitioning between eye tracking observation and input control modes.</span></span>
  
    - <span data-ttu-id="754f4-238">**Potente provider di contesto:** L'uso di informazioni su dove e cosa sta cercando l'utente durante l'esecuzione di un comando vocale o l'esecuzione di un gesto manuale consente di canalizzare senza interruzioni l'input nel campo della visualizzazione.</span><span class="sxs-lookup"><span data-stu-id="754f4-238">**Powerful context provider:** Using information about where and what the user is looking at while uttering a voice command or performing a hand gesture allows seamlessly channeling the input across the field-of-view.</span></span> <span data-ttu-id="754f4-239">Esempio: "Metti questo là" per selezionare e posizionare in modo rapido e fluido un ologramma all'interno della scena semplicemente guardando una destinazione e il punto in cui si vuole collocarlo.</span><span class="sxs-lookup"><span data-stu-id="754f4-239">For example: “Put that there” to quickly and fluently select and position a hologram across the scene by simply looking at a target and destination.</span></span> 

    - <span data-ttu-id="754f4-240">**Necessità di sincronizzare input multimodali (problematica della sezione "Uscita prima del clic"):** La combinazione di movimenti di occhi rapidi con input aggiuntivi più complessi, ad esempio comandi di tipo Long Voice o movimenti della mano, presenta il rischio di continuare a osservare gli sguardi prima di terminare il comando di input aggiuntivo.</span><span class="sxs-lookup"><span data-stu-id="754f4-240">**Need for synchronizing multimodal inputs (“leave before click” issue):** Combining rapid eye movements with more complex additional inputs, such as long voice commands or hand gestures, bears the risk of continuing your eye-gaze before finishing the additional input command.</span></span> <span data-ttu-id="754f4-241">Di conseguenza, se si creano controlli di input personalizzati (ad esempio, movimenti della mano personalizzati), assicurarsi di registrare l'inizio di questo input o la durata approssimativa per correlarla a ciò che un utente ha guardato in passato.</span><span class="sxs-lookup"><span data-stu-id="754f4-241">Hence, if you create your own input controls (e.g., custom hand gestures), make sure to log the onset of this input or approximate duration to correlate it with what a user had gazed at in the past.</span></span>
    
3. <span data-ttu-id="754f4-242">**Fornire feedback non invasivo per l'input mediante tracciamento oculare:** È utile per fornire commenti e suggerimenti quando viene esaminata una destinazione per indicare che il sistema funziona come previsto, ma deve essere mantenuto sottile.</span><span class="sxs-lookup"><span data-stu-id="754f4-242">**Subtle feedback for eye tracking input:** It's useful to provide feedback when a target is looked at to indicate that the system is working as intended but should be kept subtle.</span></span> <span data-ttu-id="754f4-243">Questo può includere la fusione lenta, in e out, le evidenziazioni visive o eseguire altri comportamenti sottili della destinazione, ad esempio movimenti lenti, ad esempio un lieve aumento delle dimensioni di destinazione, per indicare che il sistema ha rilevato correttamente che l'utente sta esaminando una destinazione senza interruzione inutilmente del flusso di lavoro corrente dell'utente.</span><span class="sxs-lookup"><span data-stu-id="754f4-243">This can include slowly blending, in and out, visual highlights or perform other subtle target behaviors, such as slow motions, such as slightly increasing the target size, to indicate that the system correctly detected that the user is looking at a target without unnecessarily interrupting the user’s current workflow.</span></span> 

4. <span data-ttu-id="754f4-244">**Evitare di applicare movimenti degli occhi innaturali come input:** Non forzare gli utenti a eseguire movimenti oculari specifici (movimenti sguardi) per attivare azioni nell'applicazione.</span><span class="sxs-lookup"><span data-stu-id="754f4-244">**Avoid enforcing unnatural eye movements as input:** Do not force users to perform specific eye movements (gaze gestures) to trigger actions in your application.</span></span>

5. <span data-ttu-id="754f4-245">**Tenere conto delle imprecisioni:** Vengono distinti due tipi di imprecisioni che sono evidenti per gli utenti: offset e jitter.</span><span class="sxs-lookup"><span data-stu-id="754f4-245">**Account for imprecisions:** We distinguish two types of imprecisions which are noticeable to users: offset and jitter.</span></span> <span data-ttu-id="754f4-246">Il modo più semplice per risolvere un offset consiste nel fornire destinazioni sufficientemente grandi per interagire con.</span><span class="sxs-lookup"><span data-stu-id="754f4-246">The easiest way to address an offset is to provide sufficiently large targets to interact with.</span></span> <span data-ttu-id="754f4-247">Si consiglia di usare un angolo visivo maggiore di 2 ° come riferimento.</span><span class="sxs-lookup"><span data-stu-id="754f4-247">It is suggested that you use a visual angle greater than 2° as a reference.</span></span> <span data-ttu-id="754f4-248">Ad esempio, l'anteprima è di circa 2 ° nell'angolo visivo quando si allunga il ARM.</span><span class="sxs-lookup"><span data-stu-id="754f4-248">For instance, your thumbnail is about 2° in visual angle when you stretch out your arm.</span></span> <span data-ttu-id="754f4-249">Segui pertanto le indicazioni seguenti:</span><span class="sxs-lookup"><span data-stu-id="754f4-249">This leads to the following guidance:</span></span>
    - <span data-ttu-id="754f4-250">Non forzare gli utenti a selezionare destinazioni minuscole.</span><span class="sxs-lookup"><span data-stu-id="754f4-250">Do not force users to select tiny targets.</span></span> <span data-ttu-id="754f4-251">La ricerca ha dimostrato che se le destinazioni sono sufficientemente grandi e che il sistema è progettato correttamente, gli utenti ne descrivono le interazioni come semplice e magico.</span><span class="sxs-lookup"><span data-stu-id="754f4-251">Research has shown that if targets are sufficiently large, and that the system is designed well, users describe their interactions as effortless and magical.</span></span> <span data-ttu-id="754f4-252">Se le destinazioni diventano troppo piccole, gli utenti descrivono l'esperienza come stancante e frustrante.</span><span class="sxs-lookup"><span data-stu-id="754f4-252">If targets become too small, users describe the experience as fatiguing and frustrating.</span></span>
   

## <a name="see-also"></a><span data-ttu-id="754f4-253">Vedere anche</span><span class="sxs-lookup"><span data-stu-id="754f4-253">See also</span></span>
* [<span data-ttu-id="754f4-254">Puntamento con la testa e commit</span><span class="sxs-lookup"><span data-stu-id="754f4-254">Head-gaze and commit</span></span>](gaze-and-commit.md)
* [<span data-ttu-id="754f4-255">Sguardo in DirectX</span><span class="sxs-lookup"><span data-stu-id="754f4-255">Head and eye-gaze in DirectX</span></span>](gaze-in-directx.md)
* [<span data-ttu-id="754f4-256">Eye-sguardi in Unity (Toolkit realtà mista)</span><span class="sxs-lookup"><span data-stu-id="754f4-256">Eye-gaze in Unity (Mixed Reality Toolkit)</span></span>](https://aka.ms/mrtk-eyes)
* [<span data-ttu-id="754f4-257">Calibrazione</span><span class="sxs-lookup"><span data-stu-id="754f4-257">Calibration</span></span>](https://docs.microsoft.com/en-us/windows/mixed-reality/calibration)
* [<span data-ttu-id="754f4-258">Movimenti della mano</span><span class="sxs-lookup"><span data-stu-id="754f4-258">Hand gestures</span></span>](gestures.md)
* [<span data-ttu-id="754f4-259">Input vocale</span><span class="sxs-lookup"><span data-stu-id="754f4-259">Voice input</span></span>](voice-design.md)
* [<span data-ttu-id="754f4-260">Controller del movimento</span><span class="sxs-lookup"><span data-stu-id="754f4-260">Motion controllers</span></span>](motion-controllers.md)
* [<span data-ttu-id="754f4-261">Comodità</span><span class="sxs-lookup"><span data-stu-id="754f4-261">Comfort</span></span>](comfort.md)
